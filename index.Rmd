---
title: "Weekly Walmart Sales Analysis"
description: |
  Utilizing SQL, R, and Python to analyze Walmart sales.
site: distill::distill_website 
output: 
  distill::distill_article


  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(tidyverse)
library(dplyr)
library(rmarkdown)
library(plotly)
library(ggplot2)
library(distill)
library(kableExtra)

```


## Welcome     

This project is currently being re-written. There are major changes in progress. First, I am looking to include more statistical analysis on how sales correlate with every factor, which means that the code and analysis of insights will be updated on a daily basis. Second, I am changing the format of this report to be more similar to a business case study. Thus, sections are being reorganized and contain imperfect writing. Third, because of the following changes, the beautification of charts and tables are currently not the main priority. Please proceed with this in mind. 

For a more polished (but less relevant and more informal case study) to showcase my data analysis skills and interest in psychology, please [click here](https://zhusophia.github.io/bigfive/intro.html)! 

## Introduction

This project focuses on analyzing raw data from [this dataset](https://www.kaggle.com/datasets/mikhail1681/walmart-sales?resource=download), which tracks the weekly sales of 45 Walmart stores over 2 years! It also tracks external factors like whether the week contained a holiday, the CPI, and the unemployment rate of the region.

First, the data is cleaned using pandas as we want to make sure that this data is easy to deal with. The primary goal of cleaning is to standardize the data by rounding values and removing missing values. 

```{python cleaning, eval=FALSE, echo=T, code_folding="Show Python code"}
#import 
import pandas as pd
df = pd.read_csv('Walmart_sales.csv', parse_dates=['Date'], dayfirst=False)

#sort by store number and then date
df.sort_values(by=['Store', 'Date'])

#standarize dates to mm-dd-yyyy
df['Date'] = pd.to_datetime(df['Date'], format='%d-%m-%Y').dt.strftime('%m-%d-%Y')

#rounding data 
df.round({'Weekly_Sales':2, 'Temperature':0, 'Fuel_Price':2, 'CPI':3, 'Unemployment':3})

#removing missing values
df.dropna

#create cleaned csv
df.to_csv('./WalmartSalesClean.csv')

```


After cleaning, the data appears like so: 

```{r load data2}
data <- read.csv("WalmartSalesClean.csv")
```

```{r general1 data}
paged_table(head(data))

```

## Analysis
Now that the data has been cleaned, we can use SQL and R to explore this data.

### Which holidays affect weekly sales the most?

Holidays are notorious for driving up sales as Walmart often runs promotional markdown events to incentivize purchases! 

```{r, holiday}

holidaydate <- data %>%
  filter(Store == 1 & Holiday_Flag == 1) %>%
  select(Date, Weekly_Sales) 

paged_table(holidaydate)
```

The (repeating) holidays that appear in this data are: 

- Super Bowl (Feb 9th)
- Labor Day (Sept 10th)
- Thanksgiving (Nov 26th)  
- Christmas and Hanukkah (Dec 31th)  

With these holidays, it is implied that the curator of data would have been American, and this data could've been taken from American stores. However, the geographical location of these stores are not contained within the dataset.  
  
First, the days had the highest profit should be examined:

```{sql test1, eval=FALSE, echo=T, code_folding="Show SQL code"}
ALTER TABLE WalmartSalesClean
ALTER COLUMN Holiday_Flag Int;

 --filtering for holiday flags and sorting 
SELECT Date, SUM(CAST(Weekly_Sales AS FLOAT)) AS Weekly_Sales
FROM WalmartSalesClean 
WHERE Holiday_Flag='1'
GROUP BY Date
ORDER BY Weekly_Sales DESC;
```

```{r, holidayordering}
holidayordering <- data %>% 
  filter(Holiday_Flag == 1) %>%
  select(Date, Weekly_Sales) %>%
  group_by(Date) %>%
  arrange(desc(Weekly_Sales))

paged_table(holidayordering)


```

It seems like Thanksgiving (for all stores) consistently ranks the highest amongst all the holidays. To be more accurate, the total sales per holiday can be extracted. 

```{sql filterholiday, eval=FALSE, echo=T, code_folding="Show SQL code"}
SELECT 
CASE 
WHEN Holiday = 02 THEN 'Super Bowl' --changing months to holiday names
WHEN Holiday = 09 THEN 'Labor Day'
WHEN Holiday = 11 THEN 'Thanksgiving'
WHEN Holiday = 12 THEN 'Christmas'
END AS Holiday, SUM(Weekly_Sales) AS Weekly_Sales
	FROM (  --subquery to have two GROUP BY statements 
	SELECT SUBSTRING (DATE, 1, 2) AS Holiday, SUM(CAST(Weekly_Sales AS FLOAT)) AS Weekly_Sales 
	FROM WalmartSalesClean 
	WHERE Holiday_Flag='1'
	GROUP BY Date
	) AS temptable 
GROUP BY Holiday --Summarizing and reordering data 
ORDER BY Weekly_Sales DESC
```

```{r, monthholidayordering}
holidaydisplay <- data %>% 
  filter(Holiday_Flag == 1) %>%
  mutate(Holiday = substr(Date, 1, 2)) %>%
  select(Holiday, Weekly_Sales) %>%
  group_by(Holiday) %>%
  summarize(Weekly_Sales = sum(Weekly_Sales)) %>%
  arrange(desc(Weekly_Sales))

holidaydisplay$Holiday <- c("Super Bowl", "Labor Day", "Thanksgiving", "Christmas")
paged_table(holidaydisplay)

```

```{r, holidaypiesales}

ggplot(holidaydisplay, aes(x="", y=Weekly_Sales, fill=Holiday)) +
  geom_bar(stat="identity", width=1, color="white") +
  ggtitle("Total Walmart Sales per Holiday")+
  coord_polar("y", start=0) +
  theme_void() 

```

Despite Thanksgiving dominating the upper echelons of highest individual store profit, it seems like people spend the most on the Super Bowl! This could be caused by the cultural significance of Thanksgiving, where it "symbolizes intercultural peace, America's opportunity for newcomers, and the sanctity of home and family".^[https://www.britannica.com/topic/Thanksgiving-Day] However, I am more inclined to believe that Thanksgiving is typically used as an opportunity to invite all your friends and family and spend the entire day cooking -- that means food costs can balloon quickly! 


### Which stores have the lowest and highest unemployment rate?

```{sql test2, eval=FALSE, echo=T, code_folding="Show SQL code"}
-- highest unemployment rate 
SELECT TOP 6 Store, avg(CAST(Unemployment AS decimal)) AS Unemployment, avg(CAST(Weekly_Sales AS decimal)) AS Weekly_Sales
FROM WalmartSalesClean
GROUP BY Store
ORDER BY Unemployment desc, Weekly_Sales desc -- need to use weekly sales as a parameter as stores may have the same unemployment rate

```

```{r employmenttable, echo=F}
highemp <- data %>%
  select(Store, Unemployment, Weekly_Sales) %>%
  group_by(Store) %>%
  summarize(Unemployment = mean(Unemployment), Weekly_Sales = mean(Weekly_Sales)) %>%
  arrange(desc(Unemployment)) 

knitr::kable(head(highemp), caption = "Highest Unemployment") %>%
  kable_styling(position = "center")


```

```{sql lowemploy, eval=FALSE, echo=T, code_folding="Show SQL code"}
-- lowest unemployment rate
SELECT TOP 6 Store, avg(CAST(Unemployment AS decimal)) AS Unemployment, avg(CAST(Weekly_Sales AS decimal)) AS Weekly_Sales
FROM WalmartSalesClean
GROUP BY Store
ORDER BY Unemployment, Weekly_Sales desc

```

```{r lowemptable, echo=F}

lowemp <- data %>%
  select(Store, Unemployment, Weekly_Sales) %>%
  group_by(Store) %>%
  summarize(Unemployment = mean(Unemployment), Weekly_Sales = mean(Weekly_Sales)) %>%
  arrange(Unemployment)

knitr::kable(head(lowemp), caption = "Lowest Unemployment") %>%
  kable_styling(position = "center")


```

Interestingly, there's not a strong correlation between high unemployment and low weekly sales. I'd expect that places with higher unemployment meant that consumers had less disposable income to spend at Walmart. But, Walmart is known for having super cheap products and consumers could've primarily bought inelastic goods and other necessities. 

It could also be that Walmart's penetration pricing worked *too* well. Walmart is notorious for significantly impacting the local economy of the neighborhood, positively and negatively. On one hand, low prices means that consumers can afford more things in a one-stop location and stores near the Walmart get more business due to the influx of foot traffic. On the other, Walmart is known for hiring many workers and paying minimum wage, as well as displacing small businesses due to their low prices (and subsequently, small profit margins) with their pricing. These margins are things that only large corporations can withstand, while small businesses simply can't afford to compete.  

The positive and negative effects differ depending on the neighborhood that Walmart enters, with this data being a good example of the impacts that Walmart can have. 

It should also be noted that this data comes from 2010 - 2012 -- aftermath of the Great Recession. Pre-recession, unemployment was at 4.7% in America. However, between 2010 - 2012, unemployement was at a high of 9.8% in January of 2009 and decreased until reaching a low of 7.7% in November 2012.^[https://www.bls.gov/charts/employment-situation/civilian-unemployment-rate.htm] 

### Sample Pearson Correlation 

We can also use statistics to back up these claims by checking the correlation between weekly sales and other factors. This gives us insight into the relationship between each of the variables. 

The sample Pearson correlation coefficient was used to determine whether there was a linear correlation between the two sets of data. The coefficient ranges from $-1$ $\le$ $r_{X,Y}$ $\le$ $1$, where $-1$ means a perfect negative linear relationship (as weekly sales go up, the other variable goes down) and $1$ means a perfect positive linear relationship (as weekly sales go up, the other variable goes up.) 

Subsequently:

- if $r_{X,Y}$ $\ge$ $0$, the variables have a positive relationship.  
- $r_{X,Y}$ $\le$ $0$, the variables have a negative relationship. 

Based on the above analysis, the following relationships are predicted: 

- Weekly sales and holiday: $r_{X,Y}$ $\ge$ $0$ 
- Weekly sales and unemployment: $r_{X,Y}$ $\le$ $0$
- Weekly sales and CPI: $r_{X,Y}$ $\ge$ $0$
- Weekly sales and fuel price: $r_{X,Y}$ $\ge$ $0$
- Weekly sales and temperature: $r_{X,Y}$ $\ge$ $0$ 

The following formula will be used:  

$r_{xy}={\frac {n\sum x_{i}y_{i}-\sum x_{i}\sum y_{i}}{\sqrt{(n\sum x^2 - \sum x \sum x)(n\sum y^2 - \sum y \sum y)}}}$

where $x$ is Weekly_Sales, and $y$ the variable we want to test the correlation of. 

The sample Pearson correlation coefficient calculated is:

```{sql samplepc, eval=FALSE, echo=T, code_folding="Show SQL code"}
-- calculating the Pearson coefficient for weekly sales and unemployment. Replacing the y value (in this case, unemployment) with the other factors would yield the coefficient for that specific factor 

WITH rdata AS (
SELECT CAST(Unemployment AS decimal) AS Unemployment, CAST(Weekly_Sales AS decimal) AS Weekly_Sales
FROM WalmartSalesClean
),

rawdata AS ( -- calculating expectation 
SELECT Unemployment AS x, Weekly_Sales AS y, Unemployment*Weekly_Sales AS xy, Unemployment*Unemployment AS xsquared, Weekly_Sales*Weekly_Sales AS ysquared	
FROM rdata
),

pdata AS ( -- reorganizing data 
SELECT SUM(x) AS sumx, SUM(y) AS sumy, SUM(xy) as sumxy, SUM(xsquared) as sumx2, SUM(ysquared) as sumy2, COUNT(x) as n
FROM rawdata
)

--- covariance(xy) / (std. dev. x) * (std. dev. y)
SELECT (n*sumxy - sumx * sumy) / (sqrt((n*sumx2 - sumx*sumx)*(n*sumy2 - sumy*sumy))) AS "Pearson's Correlation"
FROM pdata

```
```{r, pctable}
#remove dot in title name 

pctable <- data.frame(Variable = c("Unemploymment", "CPI", "Fuel Prices", "Temperature"), "Correlation Coefficient" = c(-0.101436981393985, -0.0724959823775918, -0.00646895141525588,-0.0637688579328555))

colnames(pctable) <- c("Variable", "Correlation Coefficient")


paged_table(pctable)

```

### Point-biserial Correlation Coefficient

For Holiday_Flag, we need to use a point-biserial correlation coefficient instead of the Pearson correlation coefficent. This is because Holiday_Flag is a dichotomous variable (values can only be 1 or 0 to symbolize yes/no) compared `Weekly Sales`, which is not a dichotomous variable (values can be any number). 

Although you can use Pearson's with dichotomous variables, the usefulness of the correlation would be debatable since it's not meant to be utilized with dichotomous variables. Hence, it's best to use a point-biserial, as it is equivalent to the Pearson correlation coefficient.^[https://en.wikipedia.org/wiki/Point-biserial_correlation_coefficient]

We will be using the following formula: 

${r_{pb}={\frac {M_{1}-M_{0}}{s_{n}}}{\sqrt {\frac {n_{1}n_{0}}{n^{2}}}}}$  

where:
$M_1$ = Mean weekly sales when the Holiday_Flag = 1  
$M_0$ = Mean weekly sales when the Holiday_Flag = 0  
$s_n$ = Standard deviation of weekly sales  
$n_1$ = Number of data points where Holiday_Flag = 1  
$n_2$ = Number of data points where Holiday_Flag = 0  
$n$ = Total number of data points in the sample  

```{sql, eval=FALSE, code_folding="Show SQL code"}

--grabbing avg weekly sales for holiday / no holiday
WITH meanflag AS ( 
SELECT Holiday_Flag, avg(CAST(Weekly_Sales AS decimal)) AS Avg_Weekly_Sales
FROM WalmartSalesClean
GROUP BY Holiday_Flag
), 

-- grabbing number of holiday / no holiday days
countflag AS ( 
SELECT Holiday_Flag, count(Holiday_Flag) AS n_Holiday_Flag
FROM WalmartSalesClean
GROUP BY Holiday_Flag
),

-- combining it into one table
joinmeancount AS ( 
SELECT meanflag.Holiday_Flag, meanflag.Avg_Weekly_Sales, countflag.n_Holiday_Flag
FROM meanflag
INNER JOIN countflag
	ON countflag.Holiday_Flag = meanflag.Holiday_Flag
),

-- adding standard deviation and total count of weekly sales
joinstdn AS ( 
SELECT STDEV(CAST(Weekly_Sales AS decimal)) AS sn, count(Weekly_Sales) as n, 
(SELECT n_Holiday_Flag FROM joinmeancount WHERE Holiday_Flag = 1) AS n1, 
(SELECT n_Holiday_Flag FROM joinmeancount WHERE Holiday_Flag = 0) AS n0, 
(SELECT Avg_Weekly_Sales FROM joinmeancount WHERE Holiday_Flag = 1) AS m1,
(SELECT Avg_Weekly_Sales FROM joinmeancount WHERE Holiday_Flag = 0) AS m0
FROM WalmartSalesClean
)

-- calculating correlation, need to convert into floats for division
SELECT ((m1-m0)/sn) * SQRT((CAST(n1 * n0 AS FLOAT)) / (CAST(n * n AS FLOAT))) AS r_pb 
FROM joinstdn

```

which results in an $r_pb$ value of ~ 0.037 -- a slight positive correlation. 

```{r, pctablefull}
#remove dot in title name 

pctable <- data.frame(Variable = c("Holiday", "Unemploymment", "CPI", "Fuel Prices", "Temperature"), "Correlation Coefficient" = c(0.0368880993675271, -0.101436981393985, -0.0724959823775918, -0.00646895141525588,-0.0637688579328555))

colnames(pctable) <- c("Variable", "Correlation Coefficient")
paged_table(pctable)

```

When analyzing these relationships, it seems like only Holiday and Unemployment follow the expected prediction. 


### How do Weekly Sales Vary by Store and by Year? 

Let's start by separately analyzing weekly sales. 

First, we can look at which stores are the most profitable. 

```{r, salesstorechart}

salesstore <- data %>%
  select(Store, Weekly_Sales) %>%
  group_by(Store) %>%
  summarize(Weekly_Sales = sum(Weekly_Sales)) %>%
  arrange(desc(Weekly_Sales))

ggplot(data=salesstore, aes(x=Store, y=Weekly_Sales)) +
  geom_bar(stat="identity")

```

It seems like store 33, 44, and 5 have the least amount of sales, while store 20, 4, and 14 have the most. 

We can also see how time and weekly sales affect each other.

```{r, salesyeartable}
salesyear <- data %>%
  mutate(Month = substr(Date, 1, 2), Year = substr(Date, 7, 10), Date = paste(Year,"/" ,Month)) %>%
  select(Date, Weekly_Sales) %>%
  group_by(Date) %>%
  summarize(Weekly_Sales = sum(Weekly_Sales))

ggplot(data=salesyear, aes(x=Date, y=Weekly_Sales, group=1)) +
  geom_line()+
  geom_point() + 
  theme(axis.text.x = element_text(angle = 90)) 

```
There always seems to be a spike in December and a low in January.  

<!--

However, 

```{r}

emp23 <- data %>% 
  filter(Store == 23) %>% 
  mutate(Month = substr(Date, 1, 2), Year = substr(Date, 7, 10), Date = paste(Year,"/" ,Month)) %>%
  select(Date, Unemployment) %>%
  group_by(Date) %>%
  summarize(Unemployment = mean(Unemployment))

emp23
  

ggplot(data=emp23, aes(x=Date, y=Unemployment, group=1)) +
  geom_line()+
  geom_point() + 
  theme(axis.text.x = element_text(angle = 90)) 

```

potentially uerope? 
- https://w3.unece.org/PXWeb2015/pxweb/en/STAT/STAT__20-ME__3-MELF/40_en_MEUnRateY_r.px/table/tableViewLayout1/
- https://www150.statcan.gc.ca/n1/pub/75-001-x/2010104/charts-graphiques/11148/c-g000k-eng.htm
- https://data.oecd.org/unemp/unemployment-rate.htm

We can compare it to OECD data -- 

Countries it's located in 
- US, Mexico, Canada, UK, China, South Africa, Argentina, Brazil	

https://www.thestreet.com/markets/history-of-walmart-15092339
https://grocerynews.org/research-rankings/globalization-food-sales
- https://flowingdata.com/2010/04/07/watching-the-growth-of-walmart-now-with-100-more-sams-club/

mention the economy in 2010 - 2012. is this the avg unemployment rate? what was happenign that could've influecned this? 
- how does cpi, temprature, fuel prices affect the data as well? psyhcology? 

Where are these stores located? 

Psychology of Temperature and Buying 
```{r}
tempandsales <- data %>% 
  select(Temperature, Weekly_Sales) %>%
  group_by(Temperature) %>%
  summarize(Weekly_Sales = mean(Weekly_Sales)) %>%
  arrange(desc(Weekly_Sales))

ggplot(data=tempandsales, aes(x=Temperature, y = Weekly_Sales)) +
  geom_bar(stat="identity")

```
- https://www.psychologytoday.com/ca/blog/consumed/201312/warm-and-fuzzy-temperature-and-consumer-behavior
- change labels, make it easier to understand (ex. axis labels, numbers, etc.)
box plot of temperature maybe?
```{r}

```

Pearson's correlation

```{sql test3, eval=FALSE, echo=T}
rawdata AS (
SELECT Unemployment AS x, Weekly_Sales AS y, Unemployment*Weekly_Sales AS xy, Unemployment*Unemployment AS xsquared, Weekly_Sales*Weekly_Sales AS ysquared	
FROM corrdata
),

pdata AS (
SELECT SUM(x) AS sumx, SUM(y) AS sumy, SUM(xy) as sumxy, SUM(xsquared) as sumx2, SUM(ysquared) as sumy2, COUNT(x) as n
FROM rawdata
)

SELECT (n*sumxy - sumx * sumy) / (sqrt((n*sumx2 - sumx*sumx)*(n*sumy2 - sumy*sumy))) AS "Pearson's Correlation"
FROM pdata

```

3. Is there any correlation between CPI and Weekly Sales?  How does the correlation differ when the Holiday Flag is 0 versus when the Holiday Flag is 1?
```{sql test4, eval=FALSE, echo=T}
WITH rdata AS (
SELECT CAST(ROUND(CPI, 0) AS INT) AS CPI, CAST(Weekly_Sales AS decimal) AS Weekly_Sales
FROM WalmartSalesClean
WHERE Holiday_Flag = 0
),

corrdata AS(
SELECT CPI, AVG(Weekly_Sales) AS Weekly_Sales
FROM rdata
GROUP BY CPI
/*ORDER BY CPI ASC*/
),

```

```{sql test5, eval=FALSE, echo=T}
rawdata AS (
SELECT CPI AS x, Weekly_Sales AS y, CPI*Weekly_Sales AS xy, CPI*CPI AS xsquared, Weekly_Sales*Weekly_Sales AS ysquared	
FROM corrdata
),

pdata AS (
SELECT SUM(x) AS sumx, SUM(y) AS sumy, SUM(xy) as sumxy, SUM(xsquared) as sumx2, SUM(ysquared) as sumy2, COUNT(x) as n
FROM rawdata
)

SELECT (n*sumxy - sumx * sumy) / (sqrt((n*sumx2 - sumx*sumx)*(n*sumy2 - sumy*sumy))) AS "Pearson's Correlation"
FROM pdata

```

fuel prices affect on economy: https://investopedia.com/financial-edge/0511/how-gas-prices-affect-the-economy.aspx


- Which week and which store had the highest/lowest weekly sales? What's the average?  
- sales by year, sales by store 
  - average weekly sales -> density graph? 
  - take a look at the most and least successful store 
  - what traits could impact that? take the average of cpi and stuff and see if there's a massive difference? 
  - summary of everything you learned would be very useful! 

https://www.kaggle.com/code/phuvd1609/walmart-sales-forecasting   

https://docs.google.com/spreadsheets/d/1JNzGqUo9HEn5WOKftEgcm94JddOK6G3_QW-UI4sFNUY/edit#gid=0

-->
